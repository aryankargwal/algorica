# Activation Functions
Activation functions are mathematical expressions which are attached to every neuron of an Artificial Neural Network and are responsible to determine whether said neuron should be fired depending on whether the the neuron's input is relevant for the output or the model's prediciton.<br><br>
<img src="https://miro.medium.com/fit/c/1838/551/1*pVAK4JVYrft7yRjd1K9XIg.png"><br><br>
Activation functions are a crucial part of Deep Learning and can be utilized to inspire a models accuracy, result and efficiency. Essentially being the component which can either make or break a model it is important to go for the most suitable option, so let us look at the collection of activation functions.<br><br>
We shall be covering the following Activation Functions:
- [Sigmoid](https://github.com/aryankargwal/algorica/tree/main/Activation%20Functions/Sigmoid)
- [TanH](https://github.com/aryankargwal/algorica/tree/main/Activation%20Functions/TanH)
- [ReLU](https://github.com/aryankargwal/algorica/tree/main/Activation%20Functions/ReLU)
- [Leaky ReLU](https://github.com/aryankargwal/algorica/tree/main/Activation%20Functions/Leaky%20ReLU)
- [Parametric ReLU](https://github.com/aryankargwal/algorica/tree/main/Activation%20Functions/Parametric%20ReLU)
- [Softmax](https://github.com/aryankargwal/algorica/tree/main/Activation%20Functions/Softmax)
- [Swish](https://github.com/aryankargwal/algorica/tree/main/Activation%20Functions/Swish)
